<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv='X-UA-Compatible' content='IE=edge;chrome=1' />
    <title>Jeff Lindsay</title>
    <link rel="alternate" type="application/atom+xml" title="Atom Feed" href="/blog/atom.xml" />
    <link rel="shortcut icon" type="image/x-icon" href="/images/JeffLindsayDeluxe.png" />
    <link href='http://fonts.googleapis.com/css?family=Dosis:400,300,700' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Roboto+Condensed:400,300,700' rel='stylesheet' type='text/css'>
    <link href="/stylesheets/all.css" rel="stylesheet" type="text/css" />
  </head>
  <body>
  <div id="wrapper">
    
    <header>
      <div class="container clearfix">
        <h1 id="logo">
          <a href="/">Progrium <span>// Jeff Lindsay</span></a>
        </h1>
        <nav>
          <a href="/about">About</a>
          <a href="http://github.com/progrium">Projects</a>
          <a href="/blog">Blog</a>
          <a class="hire" href="mailto:progrium+hire@gmail.com">HIRE ME</a>
        </nav>
      </div>
    </header>
    
    <main id="main" role="main">
      <section>
<div class="container">
    <div class="row">
        <div class="col-lg-9"> 
          <article>
            <div class="meta">
                <div class="date">
                    <span class="month">Aug</span>
                    <span class="day">20</span>
                    <span class="year">2014</span>
                </div>
            </div>
            <h1><a href="/blog/2014/08/20/consul-service-discovery-with-docker/">Consul Service Discovery with Docker</a></h1>
            <p><a href="http://www.consul.io/">Consul</a> is a powerful tool for building distributed systems. There are a handful of alternatives in this space, but Consul is the only one that really tries to provide a comprehensive solution for service discovery. As my <a href="/blog/2014/07/29/understanding-modern-service-discovery-with-docker/">last post</a> points out, service discovery is a little more than what Consul can provide us, but it is probably the biggest piece of the puzzle. </p>

<h2 id="understanding-consul-and-the-config-store">Understanding Consul and the "Config Store"</h2>

<p>The heart of Consul is a particular class of distributed datastore with properties that make it ideal for cluster configuration and coordination. Some call them lock servers, but I call them "config stores" since it more accurately reflects their key-value abstraction and common use for shared configuration. </p>

<p>The father of config stores is Google's Chubby, which was never made publicly available but is described in the influential <a href="http://research.google.com/archive/chubby.html">Chubby paper</a>. In the open source world we have Apache Zookeeper, the mostly defunct doozerd, and in the last year, etcd and Consul.</p>

<p>These specialized datastores are defined by their use of a consensus algorithm requiring a quorum for writes and generally exposing a simple key-value store. This key-value store is highly available, fault-tolerant, and maintains strong consistency guarantees. This can be contrasted with a number of alternative clustering approaches like master-slave or two-phase commit, all with their own benefits, drawbacks, and nuances. </p>

<p>You can learn more about the challenges of designing stateful distributed systems with the online book, <a href="http://book.mixu.net/distsys/single-page.html">Distributed systems for fun and profit</a>. This image from the book summarizes where the quorum approach stands compared to others:</p>

<p style="text-align: center;"><img src="/images/content/embassy/google-transact09.png" title="Diagram borrowed with love from Distributed systems for fun and profit" /></p>

<p>Quorum datastores such as our config stores seem to have many ideal properties <em>except</em> for performance. As a result, they're generally used as low-throughput coordinators for the rest of the system. You don't use them as your application database, but you might use them to coordinate replacing a failed database master.</p>

<p>Another common property of config stores is they all have mechanisms to watch for key-value changes in real-time. This feature is central in enabling use-cases such as electing masters, resource locking, and service presence. </p>

<h2 id="along-comes-consul">Along comes Consul</h2>

<p>Since Zookeeper came out, the subsequent config stores have been trying to simplify. Both in terms of user interface, ease of operation, and implementation of the consensus algorithms. However, they're all based on this very expressive, but lowest common denominator abstraction of a key-value store. </p>

<p>Consul is the first to build on top of this abstraction by also providing specific APIs around the semantics of common config store functions, namely service discovery and locking. It also does it in a way that's very thoughtful about those particular domains. </p>

<p>For example, a directory of services without service health is actually not a very useful one. This is why Consul also provides monitoring capabilities. Consul monitoring is comparable, and even compatible, with Nagios health checks. What's more, Consul's agent model makes it more scalable than centralized monitoring systems like Nagios.</p>

<p style="text-align: center;"><img src="/images/content/embassy/consul_layers.png" /></p>

<p>A good way to think of Consul is broken into 3 layers. The middle layer is the actual config store, which is not that different from etcd or Zookeeper. The layers above and below are pretty unique to Consul. </p>

<p>Before Consul, HashiCorp developed a host node coordinator called Serf. It uses an efficient gossip protocol to connect a set of hosts into a cluster. The cluster is aware of its members and shares an event bus. This is primarily used to know when hosts come and go from the cluster, such as during a host failure. But in Serf the event bus was also exposed for custom events to trigger user actions on the hosts.</p>

<p>Consul leverages Serf as a foundational layer to help maintain its cluster. For the most part, it's more of an implementation detail. However, I believe in an upcoming version of Consul, the Serf event bus will also be exposed in the Consul API. </p>

<p>The key-value store in Consul is very similar to etcd. It shares the same semantics and basic HTTP API, but differs in subtle ways. For example, the API for reading values lets you optionally pick a consistency mode. This is great not just because it gives users a choice, but it documents the realities of different consistency levels. This transparency educates the user about the nuances of Consul's replication model.</p>

<p>On top of the key-value store are some other great features and APIs, including locks and leader election, which are pretty standard for what people originally called lock servers. Consul is also datacenter aware, so if you're running multiple clusters, it will let you federate clusters. Nothing complicated, but it's great to have built-in since spanning multiple datacenters is very common today.</p>

<p>However, the killer feature of Consul is its service catalog. Instead of using the key-value store to arbitrarily model your service directory as you would with etcd or Zookeeper, Consul exposes a specific API for managing services. Explicitly modeling services allows it to provide more value in two main ways: monitoring and DNS.</p>

<h2 id="built-in-monitoring-system">Built-in Monitoring System</h2>

<p>Monitoring is normally discussed independent of service discovery, but it turns out to be highly related. Over the years, we've gotten better at understanding the importance of monitoring service health in relation to service discovery. </p>

<p>With Zookeeper, a common pattern for service presence, or liveness, was to have the service register an "ephemeral node" value announcing its address. As an ephemeral node, the value would exist as long as the service's TCP session with Zookeeper remained active. This seemed like a rather elegant solution to service presence. If the service died, the connection would be lost and the service listing would be dropped. </p>

<p>In the development of doozerd, the authors avoided this functionality, both for the sake of simplicity and that they believed it encouraged bad practice. The problem with relying on a TCP connection for service health is that it doesn't exactly mean the service is healthy. For example, if the TCP connection was going through a transparent proxy that accidentally kept the connection alive, the service could die and the ephemeral node may continue to exist. </p>

<p>Instead, they implemented values with an optional TTL. This allowed for the pattern of actively updating the value if the service was healthy. TTL semantics are also used in etcd, allowing the same active heartbeat pattern. Consul supports TTL as well, but primarily focuses on more robust liveness mechanisms. In the discovery layer I helped design for Flynn, our client library lets you register your service and it will automatically heartbeat for you behind the scenes. </p>

<p>This is generally effective for service presence, but it might not take the lesson to heart. Blake Mizerany, the co-author of doozerd and now maintainer of etcd, will stress the importance of <em>meaningful</em> liveness checks. In other words, there is no one-size-fits-all. Every service performs a different function and without testing that specific functionality, we don't actually know that it's working properly. Generic heartbeats can let us know if the process is running, but not that it's behaving correctly enough to safely accept connections.</p>

<p>Specialized health checks are exactly what monitoring systems give us, and Consul gives us a distributed monitoring system. Then it lets us choose if we want to want to associate a check with a service, while also supporting the simpler TTL heartbeat model as an alternative. Either way, if a service is detected as not healthy, it's hidden from queries for active services.</p>

<h2 id="built-in-dns-server">Built-in DNS Server</h2>

<p>In my last post, I mentioned how DNS is not a sufficient technology for service discovery. I was very hesitant in accepting the value of a DNS interface to services in Consul. As I described before, all our environments are set up to use DNS for resolving names to IPs, not IPs with ports. So other than identifying the IPs of hosts in the cluster, the DNS interface at first glance seems to provide limited value, if any, for our concept of service discovery. </p>

<p>However, it does serve SRV records for services, and this is huge. Built-in DNS resolvers in our environments don't lookup SRV records, however, the library support to do SRV lookups ourselves is about as ubiquitous as HTTP. This took me a while to realize. It means we all have a client, even more lightweight than HTTP, and it's made <em>specifically</em> for looking up a service. </p>

<p>To me this makes SRV the best standard API for simple service discovery lookups. I hope more service discovery systems implement it.</p>

<p>In a later post in this series, we build on SRV records from Consul DNS to generically solve service inter-connections in Docker clusters. I don't think I would have realized any of this if Consul didn't provide a built-in DNS server. </p>

<h2 id="consul-and-the-ecosystem">Consul and the Ecosystem</h2>

<p>Consul development is very active. In the past few months, they've had several significant releases, although it's still pre-1.0. Etcd is also actively being developed, though currently from the inside out, focusing on a re-design of their Raft implementation. The two projects are similar in many ways, but also very different. I hope they learn and influence each other, perhaps even share some code since they're both written in Go. At this point, though, Consul is ahead as a comprehensive service discovery primitive. </p>

<p>Unfortunately, Consul is much less popular in the Docker world. Perhaps this is just due to less of a focus on containers at HashiCorp, which is contrasted by the heavily container-oriented mindset of the etcd maintainers at CoreOS. </p>

<p>I've been trying hard to help bridge the Docker and Consul world by building a solid Consul container for Docker. I try to design containers to be self-contained, runtime-configurable appliances as much as possible. It was not hard to do this with Consul, which is now available on <a href="https://github.com/progrium/docker-consul">Github</a> or <a href="https://registry.hub.docker.com/u/progrium/consul/">Docker Hub</a>.</p>

<h2 id="running-consul-in-docker">Running Consul in Docker</h2>

<p>Running a Consul node in Docker for a production cluster can be a bit tricky. This is due to the amount of configuration that the container itself needs for Consul to work. For example, here's how you might start one node using Docker (one command over several lines for readability):</p>

<pre><code class="highlight plaintext">$ docker run --name consul -h $HOSTNAME  \
    -p 10.0.1.1:8300:8300 \
    -p 10.0.1.1:8301:8301 \
    -p 10.0.1.1:8301:8301/udp \
    -p 10.0.1.1:8302:8302 \
    -p 10.0.1.1:8302:8302/udp \
    -p 10.0.1.1:8400:8400 \
    -p 10.0.1.1:8500:8500 \
    -p 172.17.42.1:53:53/udp \
    -d -v /mnt:/data \
    progrium/consul -server -advertise 10.0.1.1 -join 10.0.1.2
</code></pre>

<p>The Consul container I built comes with a helper command letting you simply run:</p>

<pre><code class="highlight plaintext">$ $(docker run progrium/consul cmd:run 10.0.1.1::10.0.1.2 -d -v /mnt:/data)
</code></pre>

<p>This is just a special command to generate a full Docker run command like the first one, hence wrapping it in a subshell. It's not required, but a helpful convenience to hopefully get people started with Consul in Docker much quicker. </p>

<p>One of the neat ways Consul and Docker can work together is by giving Consul as a DNS server to Docker. This transparently runs DNS resolution in containers through Consul. If you set this up at the Docker daemon level, you can also specify DNS search domains. That means the <code>.services.consul</code> can be dropped, allowing containers to resolve records with just the service name. </p>

<p>The project README has some pretty helpful getting started instructions as well as more detail on all these features. Here's a quick video showing how easy it is to get a Consul cluster up and running inside Docker, including the above DNS trick.</p>

<p style="text-align: center;"><iframe src="//player.vimeo.com/video/103943481" width="640" height="360" frameborder="0" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen=""></iframe></p>

<h2 id="onward">Onward…</h2>

<p>Once you have Consul running in Docker, you're <em>close</em> to having great service discovery, but as I mentioned in my last post, you're still missing those second two legs. Stay tuned for the next post on automatically registering containerized services with Consul.</p>


          </article>
          <a href="/blog/2014/08/20/consul-service-discovery-with-docker/#disqus_thread">Comments</a>
          <br /><br />
        </div>
        
        <div class="col-lg-3" id="sidebar">
    <img src="/images/jeffphoto.gif" id="photo" title="Jeff Lindsay" />
    <p>Jeff Lindsay is a design-minded software architect and product engineer in Austin, Texas. Currently he consults and writes open source software focusing on next-gen platforms and infrastructure.</p>
    <h5><a href="http://twitter.com/progrium"><i class="fa fa-twitter-square fa-2x"></i>Twitter</a></h5>
    <h5><a href="http://github.com/progrium"><i class="fa fa-github fa-2x"></i>Github</a></h5>
    <h5><a href="http://linkedin.com/in/progrium"><i class="fa fa-linkedin-square fa-2x"></i>LinkedIn</a></h5>
    <h5><a href="http://gittip.com/progrium"><i class="fa fa-gittip fa-2x"></i>Gittip</a></h5>
</div>
    </div>
</div>
</section>


<section  class="alternate">
<div class="container">
    <div class="row">
        <div class="col-lg-9"> 
          <article>
            <div class="meta">
                <div class="date">
                    <span class="month">Jul</span>
                    <span class="day">29</span>
                    <span class="year">2014</span>
                </div>
            </div>
            <h1><a href="/blog/2014/07/29/understanding-modern-service-discovery-with-docker/">Understanding Modern Service Discovery with Docker</a></h1>
            <p>Over the next few posts, I'm going to be exploring the concepts of service discovery in modern service-oriented architectures, specifically around Docker. Many people aren't familiar with service discovery, so I have to start from the beginning. In this post I'm going to be explaining the problem and providing some historical context around solutions so far in this domain. </p>

<p>Ultimately, we're trying to get Docker containers to easily communicate across hosts. This is seen by some as one of the next big challenges in the Docker ecosystem. Some are waiting for <a href="https://en.wikipedia.org/wiki/Software-defined_networking">software-defined networking</a> (SDN) to come and save the day. I'm also excited by SDN, but I believe that well executed service discovery is the right answer today, and will continue to be useful in a world with cheap and easy software networking. </p>

<h2 id="what-is-service-discovery">What is service discovery?</h2>

<p>Service discovery tools manage how processes and services in a cluster can find and talk to one another. It involves a directory of services, registering services in that directory, and then being able to lookup and connect to services in that directory. </p>

<p>At its core, service discovery is about knowing when any process in the cluster is listening on a TCP or UDP port, and being able to look up and connect to that port by name. </p>

<p>Service discovery is a general idea, not specific to Docker, but is increasingly gaining mindshare in mainstream system architecture. Traditionally associated with <a href="https://en.wikipedia.org/wiki/Zero-configuration_networking">zero-configuration networking</a>, its more modern use can be summarized as facilitating connections to dynamic, sometimes ephemeral services.</p>

<p>This is particularly relevant today not just because of service-oriented architecture and microservices, but our increasingly dynamic compute environments to support these architectures. Already dynamic VM-based platforms like EC2 are slowly giving way to even more dynamic higher-level compute frameworks like Mesos. Docker is only contributing to this trend.</p>

<h2 id="name-resolution-and-dns">Name Resolution and DNS</h2>

<p>You might think, "Looking up by name? Sounds like DNS." Yes, name resolution is a big part of service discovery, but DNS alone is insufficient for a number of reasons. </p>

<p>A key reason is that DNS was originally not optimized for closed systems with real-time changes in name resolution. You can get away with setting TTL's to 0 in a closed environment, but this also means you need to serve and manage your own internal DNS. What highly available DNS datastore will you use? What creates and destroys DNS records for your services? Are you prepared for the archaic world of DNS RFCs and server implementations? </p>

<p>Actually, one of the biggest drawbacks of DNS for service discovery is that DNS was designed for a world in which we used standard ports for our services. HTTP is on port 80, SSH is on port 22, and so on. In that world, all you need is the IP of the host for the service, which is what an A record gives you. Today, even with private NATs and in some cases with IPv6, our services will listen on completely non-standard, sometimes random ports. Especially with Docker, we have many applications running on the same host.</p>

<p>You may be familiar with SRV records, or "service" records, which were designed to address this problem by providing the port as well as the IP in query responses. At least in terms of a data model, this brings DNS closer to addressing modern service discovery.</p>

<p>Unfortunately, SRV records alone are basically dead on arrival. Have you ever used a library or API to create a socket connection that didn't ask for the port? Where do you tell it to do an SRV record lookup? You don't. You can't. It's too late. Either software explicitly supports SRV records, or DNS is effectively just a tool for resolving names to host IPs.</p>

<p>Despite all this, DNS is still a marvel of engineering, and even SRV records will be useful to us yet. But for all these reasons, on top of the demands of building distributed systems, most large tech companies went down a different path. </p>

<h2 id="rise-of-the-lock-service">Rise of the Lock Service</h2>

<p>In 2006, Google released <a href="http://static.googleusercontent.com/media/research.google.com/en/us/archive/chubby-osdi06.pdf">a paper describing Chubby</a>, their distributed lock service. It implemented distributed consensus based on Paxos to provide a consistent, partition-tolerant (CP in CAP theorem) key-value store that could be used for coordinating leader elections, resource locking, and reliable low-volume storage. They began to use this for internal name resolution instead of DNS. </p>

<p>Eventually, the paper inspired an open source equivalent of Chubby called <a href="http://zookeeper.apache.org">Zookeeper</a> that spun out of the Hadoop Apache project. This became the de facto standard lock server in the open source world, mainly because there were no alternatives with the same properties of high availability and reliability over performance. The Paxos consensus algorithm was also non-trivial to implement.</p>

<p>Zookeeper provides similar semantics as Chubby for coordinating distributed systems, and being a consistent and highly available key-value store makes it an ideal cluster configuration store and directory of services. It's become a dependency to many major projects that require distributed coordination, including Hadoop, Storm, Mesos, Kafka, and others. Not surprisingly, it's used in mostly other Apache projects, often deployed in larger tech companies. It is quite heavyweight and not terribly accessible to "everyday" developers. </p>

<p>About a year ago, a simpler alternative to the Paxos algorithm was published called <a href="http://raftconsensus.github.io/">Raft</a>. This set the stage for a real Zookeeper alternative and, sure enough, <a href="https://github.com/coreos/etcd">etcd</a> was soon introduced by CoreOS. Besides being based on a simpler consensus algorithm, etcd is overall simpler. It's written in Go and lets you use HTTP to interact with it. I was extremely excited by etcd and used it in the initial architecture for Flynn.</p>

<p>Today there's also <a href="http://www.consul.io/">Consul</a> by Hashicorp, which builds on the ideas of etcd. I specifically explore Consul and lock servers more in my next post.</p>

<h2 id="service-discovery-solutions">Service Discovery Solutions</h2>

<p>Both Consul and etcd advertise themselves as service discovery solutions. Unfortunately, that's not entirely true. They're great service <em>directories</em>. But this is just part of a service discovery solution. So what's missing?</p>

<p>We're missing exactly how to get all our software, whether custom services or off-the-shelf software, to integrate with and use the service directory. This is particularly interesting to the Docker community, which ideally has portable solutions for anything that can run in a container.</p>

<p>A comprehensive solution to service discovery will have three legs:</p>

<ul>
  <li>A consistent (ideally), highly available service <em>directory</em></li>
  <li>A mechanism to <em>register</em> services and monitor service <em>health</em></li>
  <li>A mechanism to <em>lookup and connect</em> to services</li>
</ul>

<p>We've got good technology for the first leg, but the remaining legs, despite how they sound, aren't exactly trivial. Especially when ideally you want them to be automatic and "non-invasive." In other words, they work with non-cooperating software, not designed for a service discovery system. Luckily, Docker has both increased the demand for these properties and makes them easier to solve. </p>

<p>In a world where you have lots of services coming and going across many hosts, service discovery is extremely valuable, if not necessary. Even in smaller systems, a solid service discovery system should reduce the effort in configuring and connecting services together to nearly nothing. Adding the responsibility of service discovery to configuration management tools, or using a centralized message queue for everything are all-to-common alternatives that we know just don't scale. </p>

<p>My goal with these posts is to help you understand and arrive at a good idea of what a service discovery system should actually encompass. The next few posts will take a deeper look at each of the above mentioned legs, touching on various approaches, and ultimately explaining what I ended up doing for my soon-to-be-released project, Consulate. </p>


          </article>
          <a href="/blog/2014/07/29/understanding-modern-service-discovery-with-docker/#disqus_thread">Comments</a>
        </div>
    </div>
</div>
</section>
<section >
<div class="container">
    <div class="row">
        <div class="col-lg-9"> 
          <article>
            <div class="meta">
                <div class="date">
                    <span class="month">Jul</span>
                    <span class="day">25</span>
                    <span class="year">2014</span>
                </div>
            </div>
            <h1><a href="/blog/2014/07/25/building-the-future-with-docker/">Building the Future with Docker</a></h1>
            <p>Before Docker, a handful of colleagues and I were dealing with a typical service-oriented system on EC2. It was typical in that it was quite far from ideal. Deploying components took too long. Getting new components in production was a nightmare. It was resource inefficient and harder to manage than it needed to be. It was big and complicated, and tightly-coupled in all the wrong places. It was hard to make changes to, especially removing legacy and broken code. Every component was different. Developers could only run a few components at a time locally, which was nothing like production. System-level integration testing was near impossible. On and on…</p>

<p>We built and scaled individual components and solved individual problems to satisfy product requirements. As such, the company continued to be successful. But the system as a whole, which had the most influence on the developer and operator experience, was not improving fast enough to keep up.</p>

<p>We sat down and started to just sketch out ideas of what an ideal system would look like. What would the perfect developer experience be like? How could you keep operators from falling into reactive mode? What would facilitate developers and operators to better work together? What infrastructure would you need to employ everything we knew to be best practice? What were large-scale systems like Google and Twitter doing for these that we weren't? How could you collapse these requirements into the simplest system possible?</p>

<p>For the past three years, I've been focusing on building and refining that ideal system. The full picture is still only in my head, but over time it's been getting validated as new tools come out, either by me or by others. It's been further validated and refined as I talk and work with various companies, both providers and consumers in this space. </p>

<p>It turns out the ideal involves a different way of thinking about systems than common system architecture and operations. Though, it's not <em>so</em> different from the way modern large-scale systems like Netflix and Google have evolved. The difference is in simplicity, orthogonality, and, most important, the overall experience design. </p>

<p>I don't just have a system in mind, I have particular kind of experience with that system in mind. </p>

<p>I played a very minor role in the creation and success of Docker. To me, it was the first necessary, and maybe most pivotal component in making a reasonable approximation of this ideal system. Docker doesn't solve every problem, but it's not supposed to. It is the meta-primitive needed to build the rest of the primitives for this system. Docker is huge, but it's just the foundation.</p>

<p>I don't know how far I'll be able to take this obsessive drive to flesh out the rest of this ideal system. Luckily, I've found a lot of supporters that have sponsored my work in various ways. Without them, I would likely not even be working on these types of technologies.</p>

<p>Anyway, a couple weeks ago <a href="http://www.centurylinklabs.com/the-future-of-docker/">CenturyLink Labs did an interview with me</a> that captures a slice of the specifics in what I'm talking about. It covers some history, but it also gets into my roadmap, which I'm hoping to share more details on in upcoming posts. The big milestone for me is Manifold. As a stepping stone, the project Consulate should have a big impact in the Docker community. Some of the other projects mentioned like Duplex and Configurator are also just as important to me. </p>

<iframe width="640" height="360" src="//www.youtube.com/embed/aBgObyaNkdc" frameborder="0" allowfullscreen=""></iframe>

          </article>
          <a href="/blog/2014/07/25/building-the-future-with-docker/#disqus_thread">Comments</a>
        </div>
    </div>
</div>
</section>
<section  class="alternate">
<div class="container">
    <div class="row">
        <div class="col-lg-9"> 
          <article>
            <div class="meta">
                <div class="date">
                    <span class="month">Jul</span>
                    <span class="day">01</span>
                    <span class="year">2014</span>
                </div>
            </div>
            <h1><a href="/blog/2014/07/01/beyond-flynn-or-flynn-as-a-worldview/">Beyond Flynn, or Flynn-as-a-Worldview</a></h1>
            <p>About five months ago, I finally <a href="http://progrium.com/blog/2014/02/06/the-start-of-the-age-of-flynn/">wrote about Flynn</a>, a project started almost a year ago, but as I described was in the making for much longer. I wanted to give an update on where I am today and set the stage for some upcoming posts on what I've been working on and what I've been learning.</p>

<p>Lots has happened in five months, but first I want to talk about Flynn. There's been some confusion, so I should probably clarify that while I did help start and co-architect Flynn, it's not actually a project of mine. Jonathan and Daniel have been very considerate in allowing me to share ownership conceptually and architecturally, and they treated me as an equal partner as long as they could afford me, but technically I never was. This isn't a problem, it's just a little confusing to some. </p>

<p>Ultimately, it doesn't matter because Flynn is open source and they're going out of their way to protect the integrity of the project as open source. That's actually why I'm not in the Github organization. The project has rules about maintainers meeting an ongoing contribution requirement. Since I lost the means to work on Flynn in January, I no longer met that requirement, so I'm not listed as a maintainer.</p>

<p>Anyway, what's exciting to me are the ideas and ideals of Flynn architecturally. This is why I've continued to give talks about Flynn at conferences. While the initial goal of Flynn was to be an open source PaaS, the actual scope of Flynn was quite open-ended from there. This is because Flynn is about a new paradigm of infrastructure. </p>

<p>This new paradigm is what's been in the making for so long. It's the reason I collaborated with dotCloud on Docker, and it's the motivation behind most of my work recently, including Flynn. This paradigm is ultimately what I'm interested in figuring out and building. It's more than Docker, it's beyond Flynn. It's an ecosystem of projects that align with this worldview, that nobody owns, and maybe for the time has no name. But at least people are now starting to see it coming.</p>

<p>For the time, Flynn is the closest thing to embody this paradigm. But there are others like <a href="https://coreos.com">CoreOS</a>, and even <a href="http://deis.io">Deis</a> have aspirations towards this ideal. While my work is likely to feed back into Flynn and it's possible we'll collaborate closely again in the future, I'm not specifically working on Flynn right now. It's more like R&amp;D for Flynn and Deis and even Docker itself, forging ahead to bring us closer to this world I've been imagining for quite a while.</p>

<p>Alright, so what have I been working on? What's happened since February?</p>

<p>Not long after my last post, I decided to explore an opportunity working with <a href="http://www.digitalocean.com">DigitalOcean</a>. The general premise being, "they want to build some kind of platform layer on top of their VMs, and I can help them build that platform" using Flynn or whatever makes most sense. It seemed like a good vehicle to continue the work I had started, and work with an amazing team and great brand to take it even further.</p>

<p>As a warm-up / get-acquainted project, I decided to take on re-designing the DigitalOcean API. I drafted an initial API design, pushed it through reviews, set up infrastructure for docs and public feedback, and generally got everything moving. Eventually it was handed off since I was not getting anything else done. That API <a href="https://www.digitalocean.com/company/blog/api-v2-enters-public-beta/">entered public beta</a> last week. </p>

<p>What we learned in that time, however, was that in the growth chaos that is DigitalOcean's success, DigitalOcean was not clear on what they wanted in terms of an application platform layer. And at the time there weren't enough resources to allocate to it when there was plenty to be done with their existing cloud infrastructure. We came to an arrangement to put my employment on hold as a leave of absence for several months, coupled with sponsorship of my open source work in this space. </p>

<p>I'm absolutely thankful for this since otherwise I would not be making any progress towards this vision. Instead, I'd probably become that bitter early innovator that would have to leave the industry to avoid the pain of seeing only a shadow of what could have been…</p>

<p>As of right now, the sponsorship is a little past half over. I've been working on as much as I can in this time, but I think now would be a good time to start sharing it all with more context. If you follow me on Twitter or Github, there's been a lot of activity, and some projects have better documentation than others. I'm going to start explaining them in blog posts and hopefully that will also help convey this paradigm of tooling I'm after.</p>

<p>For now, here are links to a few of the projects built and released in this time. Most of them are components for projects I haven't gotten to yet, but that I'm excited to announce soon.</p>

<ul>
  <li><a href="https://github.com/progrium/logspout">progrium/logspout</a></li>
  <li><a href="https://github.com/progrium/busybox">progrium/busybox</a></li>
  <li><a href="https://github.com/progrium/execd">progrium/execd</a></li>
  <li><a href="https://github.com/progrium/configurator">progrium/configurator</a></li>
  <li><a href="https://github.com/progrium/nginx-appliance">progrium/nginx-appliance</a></li>
  <li><a href="https://github.com/progrium/ambassadord">progrium/ambassadord</a></li>
  <li><a href="https://github.com/progrium/docker-consul">progrium/docker-consul</a></li>
  <li><a href="https://github.com/progrium/docksul">progrium/docksul</a></li>
</ul>

<p>Lastly, Flynn is still in active development towards a stable release since their <a href="https://flynn.io/blog/flynn-preview-release">preview release in April</a>. I still contribute here and there and stay in touch with those guys. I also stay in touch with Deis, CoreOS, and more recently the Hashicorp guys. Really I talk to everybody and anybody doing anything vaguely related to what I'm after because it's important we're all moving in roughly the same direction, and I can only build on the shoulders of giants.</p>

          </article>
          <a href="/blog/2014/07/01/beyond-flynn-or-flynn-as-a-worldview/#disqus_thread">Comments</a>
        </div>
    </div>
</div>
</section>
<section >
<div class="container">
    <div class="row">
        <div class="col-lg-9"> 
          <article>
            <div class="meta">
                <div class="date">
                    <span class="month">Feb</span>
                    <span class="day">06</span>
                    <span class="year">2014</span>
                </div>
            </div>
            <h1><a href="/blog/2014/02/06/the-start-of-the-age-of-flynn/">The Start of the Age of Flynn</a></h1>
            <p>For about the past six months, I've been working on an open source project called <a href="https://flynn.io/">Flynn</a>. It's gotten a lot of attention, but I haven't written much about it. I've been hoping to start a series discussing the design and development behind Flynn, but it seems appropriate to at least introduce the project and provide some context. </p>

<h2 id="what-is-flynn">What is Flynn?</h2>

<p>Before development, I started writing the official <a href="https://github.com/flynn/flynn-guide">Flynn Guide</a>. There I explained Flynn like this:</p>

<blockquote>
  <p>Flynn has been marketed as an open source Heroku-like Platform as a Service (PaaS), however the real answer is more subtle. Flynn is two things:
<br /><br /> 
1) a "distribution" of components that out-of-the-box gives companies a reasonable starting point for an internal "platform" for running their applications and services,
<br /><br />
2) the banner for a collection of independent projects that together make up a toolkit or loose framework for building distributed systems.
<br /><br />
Flynn is both a whole and many parts, depending on what is most useful for you. The common goal is to democratize years of experience and best practices in building distributed systems. It is the software layer between operators and developers that makes both their lives easier.</p>
</blockquote>

<p>It's easy right now to describe Flynn as another, better, modern open source PaaS. But it's really much more than that. I usually need to underline this in discussions because in most people's mind, a PaaS is a black box system that you deploy and configure and then you have something like Heroku. Like you can deploy OpenStack Nova and get something like EC2. </p>

<p>Flynn can be that, but it's designed so it can be used as an open system, or framework, to build a service-oriented application operating environment. The truth is, if you're building and operating any sort of software as a service, you're not just building an application, you're building a <em>system</em> to support your application and the processes around it.</p>

<p>You might be tempted to call Flynn a tool for "devops". While that might be true, remember that the original ideas around devops were about organization-wide systemic understanding of your application, its lifecycle, and its operation. In reality, Flynn is designed for this type of thinking and should hopefully blur the line between operations and engineering, encouraging both to work together and think about the <em>entire</em> system you're building.</p>

<h2 id="why-build-flynn-how-did-it-come-about">Why build Flynn, how did it come about?</h2>

<p>This is a long story, but it provides context for the vision of Flynn, what problems inspired it, and shows just how long the idea has been stirring. Though keep in mind Flynn is a collaboration and this is just my half of the story.</p>

<h3 id="falling-in-love-with-paas">Falling in love with PaaS</h3>

<p>For years I was obsessed with improving the usefulness and programmability of our collective macro distributed system of web services and APIs. Think webhooks. Circa 2006 I was also building lots of little standalone web utilities and APIs for fun. I quickly learned that using technologies like App Engine and Heroku was imperative to sanely operate so many of these free services, and keep costs near zero for what I considered public utilities. </p>

<p>It turns out, for the exact same reasons of cost and automation, these PaaS providers were slowly revolutionizing a subset of commercial application development. The idea of generic managed applications ("NoOps") and streamlining the delivery pipeline (necessary for Continuous Delivery) has always had huge implications for web apps and their businesses. For me, even though PaaS providers couldn't have come soon enough, I seemed to always want more than what they could provide. I constantly struggled with the limitations of App Engine, Heroku, and dotCloud. Originally it was limited to certain languages, certain types of computation, certain types of protocols. In fact, there still isn't a great PaaS provider that lets you build or deploy a non-HTTP service, like say, an SMTP server or custom SSH server. </p>

<h3 id="the-divide-between-paas-and-host-based-infrastructure">The divide between PaaS and host-based infrastructure</h3>

<p>For all the systems and design knowledge, best practices, and solutions to important problems that Heroku, dotCloud, App Engine, and others have figured out, if for some reason you cannot use them, you get none if it. If it's too expensive, or your system is just too complicated, or you need to use a protocol they don't support, you just get EC2 or bare metal hosts and have to work from there. If you're lucky or smart, depending on who makes these decisions, you get to use Chef or Puppet. </p>

<p>But I'll be honest, the Chef and EC2 combo is still a huge step down from what a system like Heroku can offer. What's more is that large scale organizations like Google and Twitter have it pretty well figured out, but they have it figured out for them. The rest of us are left with a myriad of powerful if not mystical solutions to building our distributed systems like Mesos and Zookeeper. If we're lucky enough to discover and understand those projects, we often avoid them until absolutely necessary and only then figure out how to integrate them into our already complex systems. Most of which we had to build ourselves because the baseline starting point has always been "a Linux server". </p>

<h3 id="twilio-and-service-oriented-architectures">Twilio and service-oriented architectures</h3>

<p>For me, a lot of this was learned at <a href="https://www.twilio.com/">Twilio</a>, which was a perfect place to think about this space. The Twilio system, behind that wonderfully simple API, is a highly complex service-oriented system. When I left a couple years ago, it involved roughly 200 different types of services to operate. Some were off-the-shelf open source services, like databases or caches. Most of them were custom services that spoke to each other. Some were written in Python, some in PHP, some in Java, others in C. Lots of protocols were used, both internally and publicly facing. Primarily HTTP, but also SIP, RTP, custom TCP protocols, even a little ZeroMQ. Most people forget that databases also add protocols to the list.</p>

<p>I can't tell you how many problems come with a system that complicated. Though it did its job quite well, the system was effectively untestable, had an atrocious delivery pipeline, was incredibly inefficient in terms of EC2 instances, and nobody really understood the whole system. A lot of these are common problems, but they're compounded by the scale and complexity of the system. </p>

<p>The first half my time at Twilio was spent building scalable, distributed, highly-available messaging infrastructure. What a great way to learn distributed systems. However, I ran into so many problems and was so frustrated by the rest of the system, that I dedicated the second half of my time at Twilio to improving the infrastructure and helped form the Platform team. The idea being that we were to provide a service platform for the rest of the engineering organization to use. Unfortunately it never really became more than a glorified operations and systems engineering team, but we did briefly get the chance to really think through the ideal system. If we could build an ideal internal platform, what would it look like? We knew it looked a lot like Heroku, but we needed more. What we ended up with looked a lot like Flynn. But it never got started. </p>

<p>I remember very clearly working out sketches for a primitive that I thought was central to the whole system. It was a "dyno manager" to me, which gave you a utility to manage the equivalent of Heroku dynos on a single host. These were basically fancy Linux containers. Again, though, not important to Twilio at the time. Eventually, I left Twilio and started contracting.</p>

<h3 id="more-history-with-cloud-infrastructure">More history with cloud infrastructure</h3>

<p>First, I worked with some old friends back from the <a href="http://bit.ly/1fBtien">NASA Nebula</a> days. I forgot to mention, in 2009, before Twilio, I worked at NASA building open source cloud infrastructure. The plan was to implement an EC2, then a sort of App Engine on top of it, and then specifically for purposes at NASA, lots of high level application modules. Turns out the first part was hard enough. We started using Eucalyptus, but realized it was not going to cut it. Eventually the team wrote their own version from what they learned using Eucalyptus, called it Nova, partnered with Rackspace, and that's how <a href="https://www.openstack.org/">OpenStack</a> was born.</p>

<p>I was on the project pre-OpenStack to actually work on the open source PaaS back then in 2009, but we never got to that before I left. Probably for the best in terms of timing. Another reason I was there was because we also wanted to provide project hosting infrastructure at NASA. This was before Github was popular, and in fact, I was running a competing startup called DevjaVu since 2006 that productized Trac and SVN. As Github got more popular and I was distracted with other projects, I decided to shutdown DevjaVu, admitting that Github was doing it right. But my experience meant I could easily throw together what was originally going to be code.nasa.gov. </p>

<p>Fast forward to my contracting after Twilio, I worked with my friends at <a href="http://www.pistoncloud.com/">Piston Cloud</a>, one of the OpenStack startups that fell out of the NASA Nebula project. My task wasn't OpenStack related, it was actually to automate the deployment of <a href="http://www.cloudfoundry.com/">CloudFoundry</a> on top of OpenStack for a client. CloudFoundry was one of the first open source PaaS projects. It popped up in 2011. This gave me a taste of CloudFoundry, and boy it was a bad one. Ignoring anything about CloudFoundry itself, just deploying it from scratch, while 100% automated, would take 2 <em>hours</em> to complete. Nevertheless, there are still some aspects of the project I admire.</p>

<h3 id="docker-and-dokku">Docker and Dokku</h3>

<p>My next big client turned out to be an old user of DevjaVu, but I never realized it until I started talking with them. It was a company called dotCloud. Quickly hitting it off with Solomon Hykes, we tried to find a project to collaborate on. I mentioned my "dyno manager" concept, and he brought up their next-gen container technology. Soon I was working on a prototype called <a href="http://www.docker.io/">Docker</a>. </p>

<p>While the final project was mostly a product of the mind of Solomon and the team, while working on the prototype I made sure that it could be used for the systems I was envisioning. In my mind it was just one piece of a larger system, though a very powerful piece with many other uses and applications. Solomon and I knew this, and would often say it was the next big thing, but it's still a bit crazy to see that it's turning out to be true.</p>

<p>Not long after Docker was released, Solomon and I went to give a talk at GlueCon to preach this great new truth. The day before the talk, I spent 6 hours hacking together a demo for the talk that would demonstrate how you could "easily" build a PaaS with Docker. I later released this as <a href="https://github.com/progrium/dokku">Dokku</a>, a Docker powered mini-Heroku.</p>

<p>Dokku intentionally left out anything having to do with distributed systems. It was meant for a single host. I thought maybe it would be good for personal use or for building internal tools. Turns it was, and it got pretty popular. I had a note in the readme that said it intentionally does not scale to multiple hosts, and that perhaps this could be done as a separate project that I referred to as "Super Dokku". That's what I imagined Flynn as at the time.</p>

<h3 id="flynn-takes-shape">Flynn takes shape</h3>

<p>Now, back around the time I was working on the Docker prototype in Winter 2012, I was approached by two guys, Daniel Siders and Jonathan Rudenberg, that had been working on the Tent protocol and its reference implementation. They wanted to build a fully distributed, enterprise grade, open source platform service. They said they were going to be working on it soon, and they wanted to work on it with me. The only problem is they didn't have the money, and they'd get back to me after a few funding meetings.</p>

<p>Later, I think around the time I released Dokku mid-2013, Daniel and Jonathan approached me again. They were serious about this platform service project and had the idea to crowdfund it with company sponsorships. You'd think I'd be rather dubious about the idea, but given the growing interest in Docker, the great response from Dokku, and basically testimonial after testimonial of companies wanting something like this, I figured it could work.</p>

<p>We decided to call the project Flynn, and got to work comparing notes and remote whiteboarding the project. I was very lucky that they were very like-minded, and we were already thinking of very similar architectures and would generally agree on approaches. We put together the Flynn Guide and the website copy and funding campaign using <a href="http://selfstarter.us/">Selfstarter</a>, then let it loose. </p>

<p>We quickly met our funding goal for the year and then spent the rest of the year working on Flynn. Unfortunately, the budget only covered part-time, but we had planned to have a working development release by January 2014.</p>

<h2 id="what-now">What now?</h2>

<p>It's now February 2014, so let's take a look at where we are. </p>

<p>Like most software schedules, ours fell behind a little. While the project has been in the open on Github from the beginning, we planned to share a rough but usable developer release last month. We're <em>so</em> close. </p>

<p>What makes it difficult is we're out of our 2013 budget! This affects my contribution more than Jonathan's. I've been putting time into it here and there, but it no longer pays my bills. That could change soon, but until then it might move a little bit slower until our initial release. Only after the release can we can go for another sponsorship campaign, so you can see how right now is just a little frustrating.</p>

<p>That said, there's still more and more interest in the project, we already have a few brave souls that have been contributing to the project components, and like I said, hopefully the money situation will sort itself out soon. A few things are in the works. </p>

<p>In the meantime, although I'm not as active on it at this moment, I do feel compelled to use this time to write about it here on my blog. Hopefully catch everybody up on the architecture, discuss design decisions, talk about the future, and then a lot of that should be useable for official project documentation.</p>

<p>And hell, if I can't work on Flynn for some reason, after all this, hopefully the writing will allow somebody to continue my work. :)</p>

          </article>
          <a href="/blog/2014/02/06/the-start-of-the-age-of-flynn/#disqus_thread">Comments</a>
        </div>
    </div>
</div>
</section>


<section class="alternate">
<div class="container">
    <div class="row">
        <div class="col-lg-12"> 
        <ul class="pager article">
          <li class="previous">
            <a href="/blog/page2/">&larr; Older</a>
          </li>
        
          <li class="next disabled">
            <a href="#">Newer &rarr;</a>
          </li>
        </ul>
        </div>
    </div>
</div>
</section>

    </main>
    
    <!--aside>
      <h2>Recent Articles?</h2>
      <ol>
          <li><a href="/blog/2014/08/20/consul-service-discovery-with-docker/">Consul Service Discovery with Docker</a> <span>Aug 20</span></li>
          <li><a href="/blog/2014/07/29/understanding-modern-service-discovery-with-docker/">Understanding Modern Service Discovery with Docker</a> <span>Jul 29</span></li>
          <li><a href="/blog/2014/07/25/building-the-future-with-docker/">Building the Future with Docker</a> <span>Jul 25</span></li>
          <li><a href="/blog/2014/07/01/beyond-flynn-or-flynn-as-a-worldview/">Beyond Flynn, or Flynn-as-a-Worldview</a> <span>Jul  1</span></li>
          <li><a href="/blog/2014/02/06/the-start-of-the-age-of-flynn/">The Start of the Age of Flynn</a> <span>Feb  6</span></li>
          <li><a href="/blog/2013/11/13/viewdocs-hosted-markdown-project-documentation/">Viewdocs: Hosted Markdown project documentation (finally!)</a> <span>Nov 13</span></li>
          <li><a href="/blog/2013/08/18/hacker-dojo-community-trading-zone/">Hacker Dojo: Community Trading Zone</a> <span>Aug 18</span></li>
          <li><a href="/blog/2013/06/19/dokku-the-smallest-paas-implementation-youve-ever-seen/">Dokku: The smallest PaaS implementation you've ever seen</a> <span>Jun 19</span></li>
          <li><a href="/blog/2013/01/05/executable-tweets-and-programs-in-short-urls/">Executable Tweets and Programs in Short URLs</a> <span>Jan  5</span></li>
          <li><a href="/blog/2013/01/01/where-did-localtunnel-come-from/">Where did Localtunnel come from?</a> <span>Jan  1</span></li>
      </ol>

      <h2>Tags</h2>
      <ol>
      </ol>

      <h2>By Year</h2>
      <ol>
          <li><a href="/blog/2014/">2014 (5)</a></li>
          <li><a href="/blog/2013/">2013 (5)</a></li>
          <li><a href="/blog/2012/">2012 (8)</a></li>
          <li><a href="/blog/2010/">2010 (3)</a></li>
          <li><a href="/blog/2009/">2009 (5)</a></li>
          <li><a href="/blog/2007/">2007 (1)</a></li>
      </ol>
    </aside-->

    <footer>
      <div class="container clearfix">
        &copy; 2014 Jeff Lindsay, Progrium Labs
      </div>
    </footer>

  </div>
  <script src="/javascripts/all.js" type="text/javascript"></script>
  <script type="text/javascript">
        var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
        document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
        </script>
        <script type="text/javascript">
        try {
        var pageTracker = _gat._getTracker("UA-6824126-1");
        pageTracker._trackPageview();
        } catch(err) {}</script>
  </body>
</html>
